{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_sched\n",
    "import torch\n",
    "\n",
    "conv = torch.nn.Conv2d(10, 10, 3, 3)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(conv.parameters(),\n",
    "                             lr=1,\n",
    "                             weight_decay=1e-4)\n",
    "\n",
    "\n",
    "sched = lr_sched.OneCycleLR(optimizer, max_lr=0.01, total_steps=int(10**9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0003999999999999993]\n",
      "[0.0003999999999999993]\n",
      "[0.00040000000000000105]\n",
      "[0.00040000000000000105]\n",
      "[0.0004000000000000028]\n",
      "[0.00040000000000000625]\n",
      "[0.0004000000000000097]\n",
      "[0.00040000000000001146]\n",
      "[0.00040000000000001666]\n",
      "[0.00040000000000002013]\n",
      "[0.00040000000000002534]\n",
      "[0.00040000000000003054]\n",
      "[0.0004000000000000375]\n",
      "[0.0004000000000000444]\n",
      "[0.0004000000000000496]\n",
      "[0.0004000000000000583]\n",
      "[0.00040000000000006697]\n",
      "[0.00040000000000007564]\n",
      "[0.0004000000000000843]\n",
      "[0.0004000000000000947]\n",
      "[0.00040000000000010513]\n",
      "[0.00040000000000011554]\n",
      "[0.0004000000000001277]\n",
      "[0.0004000000000001381]\n",
      "[0.00040000000000015024]\n",
      "[0.0004000000000001641]\n",
      "[0.000400000000000178]\n",
      "[0.00040000000000019187]\n",
      "[0.00040000000000020575]\n",
      "[0.00040000000000022136]\n",
      "[0.00040000000000023524]\n",
      "[0.0004000000000002526]\n",
      "[0.0004000000000002682]\n",
      "[0.00040000000000028554]\n",
      "[0.0004000000000003046]\n",
      "[0.000400000000000322]\n",
      "[0.00040000000000034105]\n",
      "[0.00040000000000036014]\n",
      "[0.0004000000000003792]\n",
      "[0.00040000000000040004]\n",
      "[0.00040000000000042085]\n",
      "[0.00040000000000044167]\n",
      "[0.0004000000000004642]\n",
      "[0.00040000000000048504]\n",
      "[0.0004000000000005093]\n",
      "[0.0004000000000005319]\n",
      "[0.00040000000000055616]\n",
      "[0.00040000000000058045]\n",
      "[0.00040000000000060647]\n",
      "[0.00040000000000063075]\n",
      "[0.0004000000000006585]\n",
      "[0.0004000000000006828]\n",
      "[0.00040000000000071055]\n",
      "[0.0004000000000007383]\n",
      "[0.00040000000000076606]\n",
      "[0.00040000000000079555]\n",
      "[0.00040000000000082504]\n",
      "[0.00040000000000085453]\n",
      "[0.000400000000000884]\n",
      "[0.00040000000000091525]\n",
      "[0.0004000000000009465]\n",
      "[0.00040000000000097943]\n",
      "[0.00040000000000101066]\n",
      "[0.0004000000000010436]\n",
      "[0.0004000000000010783]\n",
      "[0.00040000000000111127]\n",
      "[0.00040000000000114597]\n",
      "[0.00040000000000118066]\n",
      "[0.0004000000000012171]\n",
      "[0.0004000000000012535]\n",
      "[0.0004000000000012882]\n",
      "[0.0004000000000013264]\n",
      "[0.0004000000000013628]\n",
      "[0.0004000000000014027]\n",
      "[0.00040000000000144087]\n",
      "[0.00040000000000147903]\n",
      "[0.00040000000000151893]\n",
      "[0.00040000000000155883]\n",
      "[0.00040000000000160046]\n",
      "[0.0004000000000016421]\n",
      "[0.00040000000000168373]\n",
      "[0.00040000000000172536]\n",
      "[0.00040000000000176873]\n",
      "[0.0004000000000018121]\n",
      "[0.00040000000000185547]\n",
      "[0.00040000000000190057]\n",
      "[0.0004000000000019457]\n",
      "[0.0004000000000019908]\n",
      "[0.0004000000000020376]\n",
      "[0.00040000000000208445]\n",
      "[0.0004000000000021313]\n",
      "[0.00040000000000217813]\n",
      "[0.0004000000000022267]\n",
      "[0.00040000000000227527]\n",
      "[0.0004000000000023256]\n",
      "[0.00040000000000237415]\n",
      "[0.00040000000000242446]\n",
      "[0.0004000000000024765]\n",
      "[0.0004000000000025268]\n",
      "[0.00040000000000257885]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mobileye/etayl/.conda/envs/bert-exp/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(sched.get_last_lr())\n",
    "    sched.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
